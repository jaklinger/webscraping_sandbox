import requests
from bs4 import BeautifulSoup
from bs4.element import NavigableString
# import nltk
# import re
from collections import defaultdict
from bs4.element import Comment
import timeout_decorator


def tag_visible(element):
    if element.parent.name in ['style', 'script', 'head', 
                               'title', 'meta', '[document]']:
        return False
    if isinstance(element, Comment):
        return False
    return True


def get_visible_text(element):
    texts = element.findAll(text=True)
    return u" ".join(t.strip() for t in
                     filter(tag_visible, texts)
                     if not t.isdigit())


def get_total_text(element):
    '''Return the total number of words in this HTML element'''
    visible_texts = get_visible_text(element)
    #return u" ".join(t.strip() for t in visible_texts)
    #s = re.sub(r'\W+', ' ', element.get_text())
    #s = nltk.word_tokenize(s)
    return len(visible_texts)


def generate_splitting_tree(element, total=None, depth=0, kill_threshold=0.1,
                            results_tree=None):
    '''Recursively generate a tree from HTML, where the nodes in the tree
    contain a fraction of `kill_threshold` of the total text (including child
    nodes) in the total HTML document'''

    # For the first run, set total amount of text and init results
    if results_tree is None:
        results_tree = defaultdict(list)
        total = get_total_text(element)

    # Iterate through children
    for child in element.children:
        # NavigableString(s) are leaf nodes, so forget about them
        if isinstance(child, NavigableString):
            continue
        # Calculate the fraction of total text in this element
        n = get_total_text(child)
        frac = n/total
        if frac < kill_threshold:
            continue
        # Update the splitting tree
        results_tree[depth+1].append(dict(info_frac=frac, element=child))
        # Dive deeper into this child node
        generate_splitting_tree(child, total, depth+1,
                                kill_threshold, results_tree)
    return results_tree


def find_best_split(results_tree):
    '''For a given results tree generated by `generate_splitting_tree`,
    find the first branch with more than one node'''
    for depth, values in results_tree.items():
        number_of_nodes = len(values)
        if number_of_nodes < 2:
            continue
        # Return the largest element, by total amount of text
        sorted_values = list(sorted(values, key=lambda x: x["info_frac"],
                                    reverse=True))
        return sorted_values[0]["element"]


@timeout_decorator.timeout(5)
def scrape(url):
    '''Find the largest split-element for the HTML of this URL'''
    r = requests.get(url)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html5lib")
    results_tree = generate_splitting_tree(soup)
    good_bit = find_best_split(results_tree)
    return good_bit


def scrape_many(labels, urls):
    '''Wrapper for `scrape`, to scrape many URLs and associate
    them concurrently with a label'''
    results = []
    for label, url in zip(labels, urls):
        exception = None
        try:
            data = scrape(url)
        except Exception as err:
            data = None
            exception = str(err)
        finally:
            results.append(dict(id=label, data=data,
                                exception=exception))
    return results


if __name__ == "__main__":
    import pandas as pd
    filename = ("/Users/jklinger/Nesta/webscraping_sandbox/"
                "feature_identification/grid.463031.3.d5_urls.csv")

    df = pd.read_csv(filename)
    for url in df.url:
        print(url)
        data = scrape(url)
        if data is not None:
            print("\n", get_visible_text(data))
        print("\n-----------------\n")
